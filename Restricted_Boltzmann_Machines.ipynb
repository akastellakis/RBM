{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "<h1>$\\textbf{Restricted Boltzmann Machines}$</h1>\n",
        "\n",
        "$\\textit{Author : Antonios Kastellakis}$<br>\n"
      ],
      "metadata": {
        "id": "b3HryxOkvn_4"
      },
      "id": "b3HryxOkvn_4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>$\\textbf{Introduction}$</h2>\n",
        "\n",
        "Restricted Boltzmann Machines (RBMs) are a special type of neural networks that was widely used as  building blocks in deep learning architectures and they continue to play an important role in applied and theoretical machine learning. In this work we will train a RBM to perform generative modeling.\n",
        "\n",
        "Generative modeling is a very useful application of neural networks that falls under the category of unsupervised learning. The model automatically discovers and learns the correlations of the input data, so as to be able to generate new instances that could have been possibly drawn from the original data set. Simply put, the model captures the distribution of the data.\n",
        "\n",
        "<h2>$\\textbf{Historical background}$</h2>\n",
        "\n",
        "In 1982 Hopfield introduced a fully connected network of interacting units that have the ability to store and retrieve binary patterns [1]. Hopfield networks can be considered as a dynamical system in which the stable states of the system correspond to the patterns we want to store. They belong to the category of energy based models where the desired patterns are associated with the minima of a suitably defined energy. In the figure below we can see a Hopfield network with 5 neurons.\n",
        "\n",
        "![Hopfield Network](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Hopfield-network.svg/247px-Hopfield-network.svg.png)\n",
        "\n",
        "The network is initialized randomly and each unit updates its state through a simple rule that depends on the units it is connected to. Mimicking nature, the evolution of a Hopfield network constantly decreases its energy. An interesting question to be raised at this point is where did the inspiration for this \"energy\" concept came from?\n",
        "\n",
        "Hopfield mentioned a spin glass system based on the Ising model [2]. In physics, a spin glass is a magnetic state characterized by randomness. In contrast with a ferromagnet where all the spins align, the spins in a spin glass are aligned randomly without a regular pattern. This is depicted in the figure below.\n",
        "\n",
        "![Spin Glass](https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Spin_glass_by_Zureks.svg/182px-Spin_glass_by_Zureks.svg.png)\n",
        "\n",
        "When a external field is applied each magnetic spin tries to align itself to the local field and in doing so it may flip. However this causes a chain reaction because it will change the fields at other dipoles. A change at the magnetic fields of other dipoles may cause them to flip which in turn changes the field at the current dipole. This process is what we call the evolution of the system and it continues until the system reaches a minimum energy state. The dipoles stop flipping if any flips results in increase of energy.\n",
        "\n",
        "The Hopfield network was very influential in the development of neural network models in the 80's. Though successful in storing/retrieving the desired patterns, it was observed that the Hopfield model has various problems such as limited storing capacity and spurious minima.  In an attempt to mitigate these issues, a stochastic version of the Hopfield model, called Boltzmann machine, was proposed [3]. In contrast to Hopfield networks, where the units are deterministic, Boltzmann machine units are stochastic. Each unit updates its state over time in a probabilistic way depending on the states of the neighboring units. The units of a Boltzmann machine are divided into 'visible' units, V, and 'hidden' units, H. The visible units are the ones that interact with the environment and work as information input to the machine, while the hidden units are latent variables forming a conditional hidden representation of the data.\n",
        "\n",
        "![Boltzmann Machine](https://upload.wikimedia.org/wikipedia/commons/7/7a/Boltzmannexamplev1.png)\n",
        "\n",
        "However, learning the parameters of the Boltzmann Machine model is computationally intensive. To reduce the complexity of learning, a restricted communication structure was introduced [4]. This model is called the Restricted Boltzmann machine.\n",
        "\n",
        "<h2>$\\textbf{RBMs}$</h2>\n",
        "\n",
        "A restricted Boltzmann machine is a probabilistic energy based model that  shares a mathematical foundation with models that describe the interactions of particles, such as the Ising model. Similar to the standard Boltzmann machine, the architecture is organised into two layers, commonly referred to as the ”visible” and ”hidden” units respectively. However, unlike Boltzmann machines, lateral connections within a layer are prohibited to make the computations faster, resulting in edges defined as $e = \\{ \\{v,h\\}: v \\in V, h \\in H \\}$. This is illustrated in the figure below.\n",
        "\n",
        "![Restricted Boltzmann machine](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/225px-Restricted_Boltzmann_machine.svg.png)\n",
        "\n",
        "The units of a restricted Boltzmann machine are stochastic binary units that can be either 0 or 1. The joint state of each layer is represented by vectors $\\mathbf{v} \\in \\{0,1\\}^V$ and $\\mathbf{h} \\in \\{0,1\\}^H$. Generally, there are other types of units that can be used, such as Gaussian units, but the most widely used are the stochastic binary units that we see here.\n",
        "\n",
        "Each unit within the RBM has an associated bias and each connection carries an associated weight. As a generative model,the RBM represents a probability distribution, and it computes the probability of observing a specific pair of a visible vector $(\\mathbf{v})$ an a hidden vector $(\\mathbf{h})$ as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "        p(\\mathbf{v},\\mathbf{h}) = \\frac{1}{Z} \\; e^{-E(\\mathbf{v},\\mathbf{h})}.\n",
        "\\end{equation}\n",
        "\n",
        "To determine the probability of a given visible sample vector $(\\mathbf{v})$, we sum over all possible hidden vectors:\n",
        "\n",
        "\\begin{equation}\n",
        "    p(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\; e^{-E(\\mathbf{v},\\mathbf{h})}.\n",
        "\\end{equation}\n",
        "\n",
        "The energy associated with each joint configuration of all units  $(\\mathbf{v},\\mathbf{h})$ depends on the pairwise interactions of the units and their biases, and it is expressed as:\n",
        "\n",
        "\\begin{align}\n",
        "    E(\\mathbf{v},\\mathbf{h}) & = - \\sum_{i \\in visible} a_i v_i \\; - \\sum_{j \\in hidden} b_j h_j \\; - \\sum_{i,j} w_{ij} v_i h_j \\\\\n",
        "    & = -\\mathbf{a}^T \\mathbf{v} - \\mathbf{b}^T \\mathbf{h} - \\mathbf{v}^T W \\mathbf{h}\n",
        "\\end{align}\n",
        "\n",
        "Here, $v_i$ and $h_j$ represent the binary states of the visible unit $i$ and hidden unit $j$, respectively. Additionally, $a_i$ and $b_j$ denote their respective biases, and $w_{ij}$ signifies the weight between these units.\n",
        "\n",
        "The 'partition function', $Z$, is calculated by summing over all possible pairs of visible and hidden vectors\n",
        "\n",
        "\\begin{equation}\n",
        "        Z = \\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}.\n",
        "\\end{equation}\n",
        "\n",
        "The partition function can be interpreted as a normalizing constant to ensure that the probabilities sum to 1. Nevertheless, lurking within this concept is a predicament. Upon closer examination, it becomes evident that computing the partition function is a computationally intensive task, especially as it involves integrating over all possible states—a feasible endeavor only for diminutive toy problems. The partition function, denoted as $Z$, exhibits exponential growth in relation to both the hidden and visible units of the model. As per complexity theory, any problem with an exponential nature should be classified as intractable when run on conventional computers.This predicament poses a significant challenge, rendering the joint probability distribution $p(\\mathbf{v},\\mathbf{h})$ intractable.\n",
        "\n",
        "The solution to this problem lies within the field of statistics with an algorithm called Gibbs sampling, which is employed when direct sampling is difficult. In statistics, Gibbs sampling is a Markov chain Monte Carlo algorithm that constructs a Markov chain whose values converge towards a target distribution. In essence, rather than calculating the joint configuration $(\\mathbf{v},\\mathbf{h})$, Gibbs sampling involves alternating transitions between states of $\\textbf{v}$ and $\\textbf{h}$ while sampling from $p(\\mathbf{h}|\\mathbf{v})$ and $p(\\mathbf{v}|\\mathbf{h})$, both of which are computationally manageable. This means that we can simultaneously and independently sample from all the elements of $\\mathbf{h}$ given $\\mathbf{v}$ and vice versa for $\\mathbf{v}$ given $\\mathbf{h}$. To get an unbiased estimate, we need the Markov chain to converge to a stationary distribution.\n",
        "\n",
        "![Markov Chain](https://miro.medium.com/v2/resize:fit:1400/1*UMbNSJVSmAgqkVnQKA62yg.png)\n",
        "\n",
        "Using Bayes theorem we can derive the probability of the hidden units conditioned on all visible units:\n",
        "\n",
        "\\begin{align}\n",
        "    p(\\mathbf{h}|\\mathbf{v}) &\n",
        "    = \\frac{p(\\mathbf{h},\\mathbf{v})}{p(\\mathbf{v})}\n",
        "    = \\frac{p(\\mathbf{v},\\mathbf{h})}{\\sum_{\\mathbf{h}} p(\\mathbf{v},\\mathbf{h}) } = \\frac{e^{-E(\\mathbf{v},\\mathbf{h})}}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})} } \\\\[0.25 cm] &\n",
        "    = \\frac{e^{\\mathbf{a}^T \\mathbf{v}}\\cdot\n",
        "    e^{\\mathbf{b}^T \\mathbf{h} +  \\mathbf{v}^T W \\mathbf{h}}}{\\sum_{\\mathbf{h}} e^{\\mathbf{a}^T \\mathbf{v}}\\cdot\n",
        "    e^{\\mathbf{b}^T \\mathbf{h} +  \\mathbf{v}^T W \\mathbf{h}}}\n",
        "    = \\frac{ e^{\\mathbf{b}^T \\mathbf{h} +  \\mathbf{v}^T W \\mathbf{h}} }{\\sum_{\\mathbf{h}}\n",
        "    e^{\\mathbf{b}^T \\mathbf{h} +  \\mathbf{v}^T W h}} \\\\[0.25 cm] &\n",
        "    = \\frac{e^{ \\sum_{j}( b_j h_j \\; + \\sum_{i} w_{ij} v_i h_j )}}{ \\sum_{\\mathbf{h}} e^{ \\sum_{j}( b_j h_j \\; + \\sum_{i} w_{ij} v_i h_j )}}\n",
        "    = \\prod_{j} \\frac{e^{ ( b_j h_j \\; + \\sum_{i} w_{ij} v_i h_j )}}{ \\sum_{\\mathbf{h}} e^{ ( b_j h_j \\; + \\sum_{i} w_{ij} v_i h_j )}}\n",
        "    \\\\[0.25 cm] &\n",
        "    = \\prod_{j} \\frac{e^{ ( b_j h_j \\; + \\sum_{i} w_{ij} v_i h_j )}}{ 1 + e^{ ( b_j  \\; + \\sum_{i} w_{ij} v_i  )}}\n",
        "    = \\prod_{j} p(h_j | \\mathbf{v}).\n",
        "\\end{align}\n",
        "\n",
        "Similarly,\n",
        "\n",
        "\\begin{equation}\n",
        "     p( \\mathbf{v} | \\mathbf{h}) = \\prod_{i} p(v_i | \\mathbf{h}).\n",
        "\\end{equation}\n",
        "\n",
        "An important point to notice here is that we have a product of probabilities. This implies conditional independence of visible units conditioned on all hidden units and vice-versa. The conditional independence is also confirmed by the bipartite structure of the model.\n",
        "\n",
        "The above conditional distributions can be expressed as\n",
        "\n",
        "\\begin{equation}\n",
        "        p(h_j=1 | \\mathbf{v}) = \\sigma \\left( b_j  \\; + \\sum_{i} w_{ij} v_i \\right) \\;\\; \\textrm{,} \\quad p(h_j=0 | \\mathbf{v}) = 1 \\; - \\; p(h_i=1 | \\mathbf{v})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "        p(v_i=1 | \\mathbf{h}) = \\sigma \\left( a_i \\; + \\sum_{j} w_{ij} h_j \\right) \\;\\; \\textrm{,} \\quad p(v_i=0 | \\mathbf{h}) = 1 \\; - \\; p(v_i=1 | \\mathbf{h})\n",
        "\\end{equation}\n",
        "\n",
        "where $\\sigma(x)$ is the sigmoid function.\n",
        "\n",
        "\n",
        "<h2>$\\textbf{Training a Restricted Boltzmann Machine}$</h2>\n",
        "\n",
        "The process of training a restricted Boltzmann machine involves determining the parameters $\\theta$ in such a way that the distribution represented by the model, denoted as $p(\\mathbf{v}|\\theta)$, closely approximates the desired distribution as indicated by the training data. The parameters, $\\theta$, of the RBM include all the connection weights and the biases.\n",
        "\n",
        "The objective is to minimize the Kullback-Leibler divergence (KL-divergence) between the model's distribution and the distribution of the training data, denoted as $p_{\\text{data}}(\\mathbf{v})$. KL-divergence is a popular measure of the distance between probability distributions and is defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "    d_{KL}( p_{data}(\\mathbf{v}) \\; || \\; p(\\mathbf{v}|\\theta))\n",
        "    = \\sum_{\\mathbf{v} \\in \\mathcal{S}} p_{data}(\\mathbf{v}) \\; log\n",
        "    \\left( \\frac{p_{data}(\\mathbf{v})}{ p(\\mathbf{v}|\\theta)}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "Here, $\\mathcal{S} = { \\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\cdots, \\mathbf{v}^{(N)}}$ represents the training samples.\n",
        "\n",
        "Interestingly, minimizing the Kullback-Leibler divergence leads to the maximization of the log-likelihood, which is expressed as:\n",
        "\n",
        "\\begin{align}\n",
        "    arg \\min\\limits_{\\theta} d_{KL} &\n",
        "    = arg \\min\\limits_{\\theta} \\sum_{\\mathbf{v} \\in \\mathcal{S}} \\left( p_{data}(\\mathbf{v}) \\; log \\, p_{data}(\\mathbf{v}) - p_{data}(\\mathbf{v}) \\; log \\, p(\\mathbf{v}|\\theta) \\right) \\\\[0.25 cm] &\n",
        "    = arg \\max\\limits_{\\theta} \\sum_{\\mathbf{v} \\in \\mathcal{S}} p_{data}(\\mathbf{v}) \\; log \\, p(\\mathbf{v}|\\theta).\n",
        "\\end{align}\n",
        "\n",
        "Typically, maximizing the log-likelihood is achieved using gradient-based optimization methods. To compute the log-likelihood of $\\theta$ for a given vector, $\\mathbf{v}$, we sum out the hidden units from the joint distribution:\n",
        "\n",
        "\\begin{align}\n",
        "    \\ln \\, p(\\mathbf{v}|\\theta) & = \\ln \\left( \\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})} \\right) \\\\[0.25 cm] &\n",
        "    = \\ln \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})} - \\ln \\sum_{\\mathbf{v} ,\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}.\n",
        "\\end{align}\n",
        "\n",
        "The gradient of the log-likelihood with respect to the parameters we want to optimize is evaluated as follows:\n",
        "\n",
        "\\begin{align}\n",
        "    \\nabla_{\\theta} \\; \\ln p(\\mathbf{v}|\\theta) &\n",
        "    = \\nabla_{\\theta} \\ln \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})} - \\nabla_{\\theta} \\ln \\sum_{\\mathbf{v} ,\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}\n",
        "    \\\\[0.25 cm] &\n",
        "    = \\frac{\\sum_{\\mathbf{h}} \\nabla_{\\theta} \\;  e^{-E(\\mathbf{v},\\mathbf{h})}}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}}\n",
        "    - \\frac{\\sum_{\\mathbf{v},\\mathbf{h}} \\nabla_{\\theta}  \\; e^{-E(\\mathbf{v},\\mathbf{h})} }{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}} \\\\[0.25 cm] &\n",
        "    = - \\frac{\\sum_{\\mathbf{h}}  e^{-E(\\mathbf{v},\\mathbf{h})} \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h}) }{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}}\n",
        "    + \\frac{\\sum_{\\mathbf{v},\\mathbf{h}}  e^{-E(\\mathbf{v},\\mathbf{h})} \\nabla_{\\theta}  \\; E(\\mathbf{v},\\mathbf{h}) }{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h})}} \\\\[0.25 cm] &\n",
        "    = - \\sum_{\\mathbf{h}} p(\\mathbf{h} | \\mathbf{v}) \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h}) + \\sum_{\\mathbf{v},\\mathbf{h}} p(\\mathbf{v},\\mathbf{h}) \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h})\n",
        "\\end{align}\n",
        "\n",
        "For a given set of training samples, $\\mathcal{S} = { \\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\cdots, \\mathbf{v}^{(N)}}$, the gradient of the log-likelihood is:\n",
        "\n",
        "\\begin{align}\n",
        "    \\frac{1}{N} \\sum_{l=1}^N \\nabla_{\\theta} \\; \\ln p(\\mathbf{v}^{(l)}|\\theta) &\n",
        "    = \\frac{1}{N} \\sum_{l=1}^N \\left( - \\sum_{\\mathbf{h}} p(\\mathbf{h} | \\mathbf{v}) \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h}) + \\sum_{\\mathbf{v},\\mathbf{h}} p(\\mathbf{v},\\mathbf{h}) \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h})\\right)\n",
        "    \\\\[0.25 cm] &\n",
        "    = \\mathbb{E}_{p_{model}} \\left[ \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h})\\right]\n",
        "    - \\mathbb{E}_{p_{data}} \\left[ \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h})\\right].\n",
        "\\end{align}\n",
        "\n",
        "The updates for weights and biases are as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\Delta w_{ij} & = \\mathbb{E}_{p{\\text{data}}} \\left[ v_i h_j\\right]\n",
        "- \\mathbb{E}_{p{\\text{model}}} \\left[ v_i h_j \\right] \\\\[0.25 cm]\n",
        "\\Delta a_{i} & = \\mathbb{E}_{p{\\text{data}}} \\left[ v_i\\right]\n",
        "- \\mathbb{E}_{p{\\text{model}}} \\left[ v_i \\right]\\\\[0.25 cm]\n",
        "\\Delta b_{j} & = \\mathbb{E}_{p{\\text{data}}} \\left[ h_j\\right]\n",
        "- \\mathbb{E}_{p{\\text{model}}} \\left[ h_j \\right].\n",
        "\\end{align}\n",
        "\n",
        "The first term can be easily obtained and is referred to as the \"positive phase.\" It calculates the expectation of the hidden probabilities given the data under the current model. The second term, known as the \"negative phase,\" computes the expectation of the joint probability of visible and hidden units under the current model. However, the second term exhibits exponential growth in the size of the smallest layer, $2^{\\min(m,n)}$. The following equations shed light on why this is the case:\n",
        "\n",
        "\\begin{align}\n",
        "    \\sum_{\\mathbf{v},\\mathbf{h}} p(\\mathbf{v},\\mathbf{h}) \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h}) & =\n",
        "    \\sum_{\\mathbf{v}} p(\\mathbf{v}) \\sum_{\\mathbf{h}} p(\\mathbf{h}|\\mathbf{v})\n",
        "    \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h}) \\\\\n",
        "    & = \\sum_{\\mathbf{h}} p(\\mathbf{h}) \\sum_{\\mathbf{v}} p(\\mathbf{v}|\\mathbf{h})\n",
        "    \\nabla_{\\theta} \\; E(\\mathbf{v},\\mathbf{h})\n",
        "\\end{align}\n",
        "\n",
        "Thus, once again, Markov chain Monte Carlo sampling methods are employed to obtain expectations under the model distribution. These samples are obtained when the Markov chain converges to the stationary distribution. However, should we wait for the chain to converge at each iteration, the computational cost becomes large again. To mitigate this computational burden, an algorithm known as \"Contrastive Divergence\" is used [5].\n",
        "\n",
        "The idea is to reduce the computational cost by initializing the Markov chain close to the desired distribution. This is achieved by initializing the chain with samples from the data set. Moreover, the expectation is replaced by a single sample, denoted as $v^{k}$, which is obtained after running a Markov chain for $k$ steps. This approach significantly reduces the computational overhead required to obtain samples from the fully converged distribution.\n",
        "\n",
        "According to Hinton, we can interpret 'contrastive divergence' as the discrepancy between two KL-divergences. Instead of merely minimizing the KL-divergence between the data distribution and the fully converged Markov chain distribution, we aim to minimize the following expression:\n",
        "\n",
        "\\begin{equation}\n",
        "    d_{KL}( p_{data}(\\mathbf{v}) \\; || \\; p(\\mathbf{v}|\\theta)) =\n",
        "    d_{KL}( p_{data}(\\mathbf{v}) \\; || \\; p(\\mathbf{v}|\\theta)) -\n",
        "    d_{KL}( p_{k}(\\mathbf{v}) \\; || \\; p(\\mathbf{v}|\\theta))\n",
        "\\end{equation}\n",
        "\n",
        "Here, $p_{k}(\\mathbf{v})$ represents the distribution of the chain after undergoing k steps. In practical scenarios, just a single step of Gibbs sampling is typically sufficient for most problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "xlGrQ5-MwApC"
      },
      "id": "xlGrQ5-MwApC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>$\\textbf{Coding our own RBM}$</h2>\n",
        "\n",
        "This implementation is done based on Hinton's practical guide [6]."
      ],
      "metadata": {
        "id": "x935ZHIZd-oE"
      },
      "id": "x935ZHIZd-oE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "h44R1DKne330"
      },
      "id": "h44R1DKne330",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for a given input.\n",
        "\n",
        "    Parameters:\n",
        "    z (float or numpy.ndarray): Input value(s) for which to compute the sigmoid.\n",
        "\n",
        "    Returns:\n",
        "    float or numpy.ndarray: Sigmoid of the input(s).\n",
        "    \"\"\"\n",
        "    sig = 1.0 / (1.0 + np.exp(-z))\n",
        "    return sig"
      ],
      "metadata": {
        "id": "NrNZ0LC3fNm_"
      },
      "id": "NrNZ0LC3fNm_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM(object):\n",
        "\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        \"\"\"\n",
        "        Initialize a Restricted Boltzmann Machine (RBM) with the given number of visible and hidden neurons.\n",
        "\n",
        "        Parameters:\n",
        "        num_visible (int): Number of neurons in the visible layer.\n",
        "        num_hidden (int): Number of neurons in the hidden layer.\n",
        "        \"\"\"\n",
        "        # The number of neurons in the visible layer.\n",
        "        self.num_visible = num_visible\n",
        "        # The number of neurons in the hidden layer.\n",
        "        self.num_hidden = num_hidden\n",
        "        # Initialize biases with a Gaussian distribution (mean 0, std 1).\n",
        "        self.a = np.zeros((1, num_visible))\n",
        "        self.b = np.ones((1, num_hidden))\n",
        "        # Initialize weights with a Gaussian distribution (mean 0, std 0.01).\n",
        "        self.weights = 0.01 * np.random.randn(num_visible, num_hidden)\n",
        "        # Parameters for implementing momentum in training.\n",
        "        self.w_inc = np.zeros(self.weights.shape)\n",
        "        self.a_inc = np.zeros(self.a.shape)\n",
        "        self.b_inc = np.zeros(self.b.shape)\n",
        "\n",
        "    def train_RBM(self, training_data, epochs, mini_batch_size, eta):\n",
        "        \"\"\"\n",
        "        Train the RBM using Contrastive Divergence.\n",
        "\n",
        "        Parameters:\n",
        "        training_data (numpy.ndarray): Training data as a matrix.\n",
        "        epochs (int): Number of training epochs.\n",
        "        mini_batch_size (int): Size of mini-batches for training.\n",
        "        eta (float): Learning rate.\n",
        "        \"\"\"\n",
        "        # Training to create a generative model.\n",
        "        for epoch in range(epochs):\n",
        "            # Randomly shuffle the training data and partition it into mini-batches.\n",
        "            error = 0\n",
        "            training_data_size = len(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, training_data_size, mini_batch_size)]\n",
        "            for batch in mini_batches:\n",
        "                err = self.Contrastive_Divergence(batch, mini_batch_size, epoch, epochs, eta)\n",
        "                error += err\n",
        "            print(\"Epoch %s complete. Reconstruction error is %0.2f\" % (epoch+1, error))\n",
        "\n",
        "        print(\"Training completed.\\n\")\n",
        "\n",
        "    def Contrastive_Divergence(self, v0, mini_batch_size, epoch, epochs, eta):\n",
        "        \"\"\"\n",
        "        Update the network's weights and biases with mini-batch Contrastive Divergence.\n",
        "\n",
        "        Parameters:\n",
        "        v0 (numpy.ndarray): Mini-batch of training data.\n",
        "        mini_batch_size (int): Size of the mini-batch.\n",
        "        epoch (int): Current training epoch.\n",
        "        epochs (int): Number of training epochs.\n",
        "        eta (float): Learning rate.\n",
        "\n",
        "        Returns:\n",
        "        float: Reconstruction error.\n",
        "        \"\"\"\n",
        "        # Positive phase of contrastive divergence (data-driven phase).\n",
        "        z_h0 = np.dot(v0, self.weights) + np.tile(self.b, (v0.shape[0], 1))\n",
        "        prob_h0 = sigmoid(z_h0)\n",
        "        h0 = prob_h0 > np.random.rand(v0.shape[0], self.num_hidden)\n",
        "        vihj_data = np.dot(v0.T, prob_h0)\n",
        "        vi_data = np.sum(v0, axis=0)\n",
        "        hj_data = np.sum(prob_h0, axis=0)\n",
        "\n",
        "        # Negative phase of contrastive divergence (reconstruction-driven phase).\n",
        "        z_v1 = np.dot(h0, self.weights.T) + np.tile(self.a, (h0.shape[0], 1))\n",
        "        v1 = sigmoid(z_v1)\n",
        "        z_h1 = np.dot(v1, self.weights) + np.tile(self.b, (v1.shape[0], 1))\n",
        "        prob_h1 = sigmoid(z_h1)\n",
        "        vihj_model = np.dot(v1.T, prob_h1)\n",
        "        vi_model = np.sum(v1, axis=0)\n",
        "        hj_model = np.sum(prob_h1, axis=0)\n",
        "\n",
        "        # Decrease learning rate at the end of training as per Hinton's practical guide.\n",
        "        eta = eta/mini_batch_size if epoch > (0.8 * epochs) else eta\n",
        "        # Momentum as per Hinton's suggestion.\n",
        "        m = 0.5 if epoch > 5 else 0.9\n",
        "        self.w_inc = self.w_inc * m + (eta / mini_batch_size) * (vihj_data - vihj_model)\n",
        "        self.a_inc = self.a_inc * m + (eta / mini_batch_size) * (vi_data - vi_model)\n",
        "        self.b_inc = self.b_inc * m + (eta / mini_batch_size) * (hj_data - hj_model)\n",
        "\n",
        "        # Updating weights and biases\n",
        "        self.weights += self.w_inc\n",
        "        self.a += self.a_inc\n",
        "        self.b += self.b_inc\n",
        "\n",
        "        err = np.sum((v0 - v1) ** 2)\n",
        "\n",
        "        return err\n",
        "\n",
        "    def Reconstruct(self, V_data, recon_steps):\n",
        "        \"\"\"\n",
        "        Reconstruct the input vector using Gibbs sampling.\n",
        "\n",
        "        Parameters:\n",
        "        V_data (numpy.ndarray): Input data to be reconstructed.\n",
        "        recon_steps (int): Number of Gibbs sampling steps.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Reconstructed input vector.\n",
        "        \"\"\"\n",
        "        V_rec = np.copy(V_data)\n",
        "\n",
        "        for i in range(recon_steps):\n",
        "            z = np.dot(V_rec, self.weights) + self.b\n",
        "            prob_h = sigmoid(z)\n",
        "            h = prob_h > np.random.rand(1, self.num_hidden)\n",
        "\n",
        "            z_v1 = np.dot(h, self.weights.T) + self.a\n",
        "            V_rec = sigmoid(z_v1)\n",
        "\n",
        "        return V_rec\n"
      ],
      "metadata": {
        "id": "r7sUmNsrgBd1"
      },
      "id": "r7sUmNsrgBd1",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>$\\textbf{Learning the MNIST digits}$</h2>"
      ],
      "metadata": {
        "id": "FZa47CGrSU4S"
      },
      "id": "FZa47CGrSU4S"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "441b3a1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "441b3a1c",
        "outputId": "5f2fecf0-e668-4cd4-a8c8-01459c9b303a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1 complete. Reconstruction error is 1927873.00\n",
            "Epoch 2 complete. Reconstruction error is 1564295.83\n",
            "Epoch 3 complete. Reconstruction error is 1489237.23\n",
            "Epoch 4 complete. Reconstruction error is 1452821.79\n",
            "Epoch 5 complete. Reconstruction error is 1432376.58\n",
            "Epoch 6 complete. Reconstruction error is 1409968.29\n",
            "Epoch 7 complete. Reconstruction error is 1120014.01\n",
            "Epoch 8 complete. Reconstruction error is 1003657.92\n",
            "Epoch 9 complete. Reconstruction error is 946877.26\n",
            "Epoch 10 complete. Reconstruction error is 907115.70\n",
            "Epoch 11 complete. Reconstruction error is 873771.40\n",
            "Epoch 12 complete. Reconstruction error is 848170.67\n",
            "Epoch 13 complete. Reconstruction error is 826264.84\n",
            "Epoch 14 complete. Reconstruction error is 808848.81\n",
            "Epoch 15 complete. Reconstruction error is 793532.37\n",
            "Epoch 16 complete. Reconstruction error is 779822.22\n",
            "Epoch 17 complete. Reconstruction error is 767911.33\n",
            "Epoch 18 complete. Reconstruction error is 755998.86\n",
            "Epoch 19 complete. Reconstruction error is 745733.60\n",
            "Epoch 20 complete. Reconstruction error is 735818.70\n",
            "Epoch 21 complete. Reconstruction error is 727353.50\n",
            "Epoch 22 complete. Reconstruction error is 719330.52\n",
            "Epoch 23 complete. Reconstruction error is 711312.83\n",
            "Epoch 24 complete. Reconstruction error is 704514.06\n",
            "Epoch 25 complete. Reconstruction error is 696324.64\n",
            "Epoch 26 complete. Reconstruction error is 689781.61\n",
            "Epoch 27 complete. Reconstruction error is 684016.74\n",
            "Epoch 28 complete. Reconstruction error is 677949.26\n",
            "Epoch 29 complete. Reconstruction error is 672723.15\n",
            "Epoch 30 complete. Reconstruction error is 667422.13\n",
            "Epoch 31 complete. Reconstruction error is 661396.81\n",
            "Epoch 32 complete. Reconstruction error is 656192.50\n",
            "Epoch 33 complete. Reconstruction error is 652756.28\n",
            "Epoch 34 complete. Reconstruction error is 647181.89\n",
            "Epoch 35 complete. Reconstruction error is 643020.91\n",
            "Epoch 36 complete. Reconstruction error is 639637.64\n",
            "Epoch 37 complete. Reconstruction error is 635384.97\n",
            "Epoch 38 complete. Reconstruction error is 631196.36\n",
            "Epoch 39 complete. Reconstruction error is 627024.99\n",
            "Epoch 40 complete. Reconstruction error is 623492.08\n",
            "Epoch 41 complete. Reconstruction error is 620872.22\n",
            "Epoch 42 complete. Reconstruction error is 548019.58\n",
            "Epoch 43 complete. Reconstruction error is 534142.56\n",
            "Epoch 44 complete. Reconstruction error is 530280.84\n",
            "Epoch 45 complete. Reconstruction error is 527495.79\n",
            "Epoch 46 complete. Reconstruction error is 525862.56\n",
            "Epoch 47 complete. Reconstruction error is 523957.95\n",
            "Epoch 48 complete. Reconstruction error is 522988.40\n",
            "Epoch 49 complete. Reconstruction error is 522634.52\n",
            "Epoch 50 complete. Reconstruction error is 520985.16\n",
            "Training completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# LOADING MNIST dataset.\n",
        "# We can verify that the split between train and test is 60,000 and 10,000, respectively.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# Convert data to float32.\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Reshape the data.\n",
        "# X_train is 60,000 rows of 28 x 28 values, reshape it to 60,000 x 784.\n",
        "# X_test is 10,000 rows of 28 x 28 values, reshape it to 10,000 x 1 x 784.\n",
        "training_data = np.reshape(X_train, (X_train.shape[0], 784))\n",
        "test_data = np.reshape(X_test, (X_test.shape[0], 784))\n",
        "\n",
        "# Normalize inputs to be within [0, 1] and convert to binary data.\n",
        "threshold = 128\n",
        "training_data = (training_data > np.ones(training_data.shape) * threshold) * 1\n",
        "test_data = (test_data > np.ones(test_data.shape) * threshold) * 1\n",
        "\n",
        "# Define the size and structure of the RBM\n",
        "Num_visible_units = 784\n",
        "Num_hidden_units = 300\n",
        "\n",
        "# Initialize an RBM instance\n",
        "rbm = RBM(Num_visible_units, Num_hidden_units)\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 50\n",
        "# For datasets that contain a small number of equiprobable classes, the ideal mini-batch size is often\n",
        "# equal to the number of classes\n",
        "BATCH_SIZE = 10\n",
        "# It is helpful to divide the total gradient computed on a mini-batch by the size of the mini-batch,\n",
        "# so when talking about learning rates we will assume that they multiply the average,\n",
        "# per-case gradient computed on a mini-batch, not the total gradient for the mini-batch.\n",
        "ETA = 1 / BATCH_SIZE\n",
        "\n",
        "# Train the RBM model\n",
        "rbm.train_RBM(training_data, EPOCHS, BATCH_SIZE, ETA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c644219c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "c644219c",
        "outputId": "7ceb4b72-d4f7-4ad7-ffd6-0b3048ba73f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Compare the initial image with its reconstructions\n",
        "\n",
        "# Number of examples\n",
        "examples = 3\n",
        "\n",
        "# Number of steps through alternating Gibbs Markov chain\n",
        "recon_steps = 1\n",
        "\n",
        "# Create a figure and a grid of subplots for displaying images\n",
        "figure, img = plt.subplots(examples, 4)\n",
        "\n",
        "# Set titles for the subplots\n",
        "img[0, 0].set_title('Original')\n",
        "img[0, 1].set_title('Recon 1')\n",
        "img[0, 2].set_title('Recon 2')\n",
        "img[0, 3].set_title('Recon 3')\n",
        "\n",
        "for i in range(examples):\n",
        "    # Select a random digit to reconstruct\n",
        "    digit = np.random.randint(0, X_train.shape[0] - 1)\n",
        "\n",
        "    original_image = np.reshape(training_data[digit], (28, 28)) * 255\n",
        "\n",
        "    # Display the original image in the first column\n",
        "    img[i, 0].imshow(original_image, cmap='gray')\n",
        "\n",
        "    rec1 = rbm.Reconstruct(training_data[digit], recon_steps)\n",
        "    rec1_img = np.reshape(rec1, (28, 28)) * 255\n",
        "    # Display the first reconstruction in the second column\n",
        "    img[i, 1].imshow(rec1_img, cmap='gray')\n",
        "\n",
        "    rec2 = rbm.Reconstruct(rec1, recon_steps)\n",
        "    rec2_img = np.reshape(rec2, (28, 28)) * 255\n",
        "    # Display the second reconstruction in the third column\n",
        "    img[i, 2].imshow(rec2_img, cmap='gray')\n",
        "\n",
        "    rec3 = rbm.Reconstruct(rec2, recon_steps)\n",
        "    rec3_img = np.reshape(rec3, (28, 28)) * 255\n",
        "    # Display the third reconstruction in the fourth column\n",
        "    img[i, 3].imshow(rec3_img, cmap='gray')\n",
        "\n",
        "# Adjust layout and display the figure\n",
        "figure.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. [\"Neural networks and physical systems with emergent collective computational abilities\", JJ Hopfield.](https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554)\n",
        "\n",
        "2. [“History of the Lenz-Ising model”, Stephen G Brush.](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.39.883)\n",
        "\n",
        "3. [\"A learning algorithm for Boltzmann machines\",\n",
        "DH Ackley, GE Hinton, TJ Sejnowski.](https://www.sciencedirect.com/science/article/abs/pii/S0364021385800124)\n",
        "\n",
        "4. [\"Information processing in dynamical systems: Foundations\n",
        "of harmony theory\", Paul Smolensky, Tech. rep. Colorado Univ at Boulder Dept of Computer Science, 1986](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap6_PDP86.pdf)\n",
        "\n",
        "5. [\"Training products of experts by minimizing contrastive divergence\",\n",
        "GE Hinton.](https://ieeexplore.ieee.org/abstract/document/6789337)\n",
        "\n",
        "6. [\"A practical guide to training restricted Boltzmann machines\",\n",
        "GE Hinton.](https://www.csrc.ac.cn/upload/file/20170703/1499052743888438.pdf)"
      ],
      "metadata": {
        "id": "435XM2RoxGZd"
      },
      "id": "435XM2RoxGZd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}